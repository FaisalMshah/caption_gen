{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import cv2\n",
    "import random\n",
    "import numpy as np\n",
    "import random"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "SEED=1234\n",
    "np.random.seed(SEED)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from torch.utils import data\n",
    "import torch.nn as nn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from dataloader import Dataset\n",
    "from vocabulary import Vocabulary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset_folder=\"/floyd/input/bangla_image_caption/\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install bnltk"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset_folder=\"/floyd/input/bangla_image_caption/\"\n",
    "\n",
    "caption_json_path=dataset_folder+\"captions.json\"\n",
    "\n",
    "filenames_with_captions=json.load(open(caption_json_path))\n",
    "\n",
    "from bnltk.tokenize import Tokenizers\n",
    "t = Tokenizers()\n",
    "tokenizer = t.bn_word_tokenizer\n",
    "\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "train,test=train_test_split(filenames_with_captions,test_size=0.1)\n",
    "train,valid=train_test_split(filenames_with_captions,test_size=0.1)\n",
    "\n",
    "image_names=[]\n",
    "all_captions=[]\n",
    "train_with_captions=[]\n",
    "val_with_captions=[]\n",
    "test_with_captions=[]\n",
    "image_folder= dataset_folder+\"images/\"\n",
    "for filename_caption in train:\n",
    "    image_name = filename_caption[\"filename\"]\n",
    "    captions=filename_caption[\"caption\"]\n",
    "    for caption in captions:\n",
    "        train_with_captions.append((image_name,caption))\n",
    "        all_captions.append(caption)\n",
    "for filename_caption in valid:\n",
    "    image_name = filename_caption[\"filename\"]\n",
    "    captions=filename_caption[\"caption\"]\n",
    "    for caption in captions:\n",
    "        val_with_captions.append((image_name,caption))\n",
    "        all_captions.append(caption)\n",
    "for filename_caption in test:\n",
    "    image_name = filename_caption[\"filename\"]\n",
    "    captions=filename_caption[\"caption\"]\n",
    "    for caption in captions:\n",
    "        test_with_captions.append((image_name,caption))\n",
    "        all_captions.append(caption)\n",
    "\n",
    "vocab=Vocabulary(vocab_threshold=6,captions=all_captions,tokenizer=t.bn_word_tokenizer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torchvision import transforms\n",
    "\n",
    "# Define a transform to pre-process the training images.\n",
    "transform_train = transforms.Compose([ \n",
    "    transforms.Resize(224),                           # get 224x224 crop from random location\n",
    "    transforms.RandomHorizontalFlip(),               # horizontally flip image with probability=0.5\n",
    "    transforms.ToTensor(),                           # convert the PIL Image to a tensor\n",
    "])\n",
    "transform_test = transforms.Compose([ \n",
    "    transforms.Resize(224),                          # smaller edge of image resized to 256\n",
    "    transforms.ToTensor(),                           # convert the PIL Image to a tensor\n",
    "])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset= Dataset(image_folder,train_with_captions,transform_train,vocab,tokenizer=t.bn_word_tokenizer)\n",
    "valid_dataset= Dataset(image_folder,val_with_captions,transform_test,vocab,tokenizer=t.bn_word_tokenizer)\n",
    "test_dataset= Dataset(image_folder,test_with_captions,transform_test,vocab,tokenizer=t.bn_word_tokenizer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from model import EncoderCNN,EncoderVGG,EncoderVGGAtt\n",
    "from model import DecoderRNN,DecoderRNNAttention"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "batch_size = 128          # batch size\n",
    "vocab_threshold = 6        # minimum word count threshold\n",
    "vocab_from_file = True    # if True, load existing vocab file\n",
    "embed_size = 4096         # dimensionality of image and word embeddings\n",
    "hidden_size = 512          # number of features in hidden state of the RNN decoder\n",
    "num_epochs = 20             # number of training epochs (1 for testing)\n",
    "save_every = 1             # determines frequency of saving model weights\n",
    "print_every = 200          # determines window for printing average loss\n",
    "log_file = 'training_log_attention.txt'       # name of file with saved training loss and perplexity\n",
    "val_log_file = 'validation_log_attention.txt'\n",
    "vocab_size=len(vocab)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "encoder = EncoderVGG()\n",
    "decoder = DecoderRNN(embed_size, hidden_size, vocab_size,batch_size)\n",
    "\n",
    "# Move models to GPU if CUDA is available. \n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "encoder.to(device)\n",
    "decoder.to(device)\n",
    "\n",
    "# Define the loss function. \n",
    "criterion = nn.CrossEntropyLoss().cuda() if torch.cuda.is_available() else nn.CrossEntropyLoss()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "params = list(decoder.parameters())# + list(encoder.embed.parameters()) \n",
    "optimizer = torch.optim.Adam(params, lr=0.001,weight_decay=0.001)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "validation_loss_min = np.inf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_losses=[]\n",
    "validation_losses=[]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "f = open(log_file, 'w')\n",
    "validation_f=open(val_log_file,'w')\n",
    "i_step=0\n",
    "print_every=100\n",
    "\n",
    "for epoch in range(1, num_epochs+1):\n",
    "    try:\n",
    "        for images,captions in dataset.load_data(batch_size):\n",
    "            images=torch.cat(images)\n",
    "\n",
    "\n",
    "            # Move batch of images and captions to GPU if CUDA is available.\n",
    "            images_gpu = images.to(device)\n",
    "            captions_gpu = [caption.to(device) for caption in captions]\n",
    "            captions_padded=nn.utils.rnn.pad_sequence(captions_gpu,batch_first=True)\n",
    "            # Zero the gradients.\n",
    "            decoder.zero_grad()\n",
    "            encoder.zero_grad()\n",
    "\n",
    "            # Pass the inputs through the CNN-RNN model.\n",
    "            features = encoder(images_gpu)\n",
    "            outputs = decoder(features, captions_gpu)\n",
    "\n",
    "            # Calculate the batch loss.\n",
    "    #         print(\"outputs.shape: \", outputs.shape)\n",
    "            loss = criterion(outputs.contiguous().view(-1, vocab_size), captions_padded.view(-1))\n",
    "\n",
    "            # Backward pass.\n",
    "            loss.backward()\n",
    "\n",
    "            # Update the parameters in the optimizer.\n",
    "            optimizer.step()\n",
    "\n",
    "            # Get training statistics.\n",
    "            stats = 'Epoch [%d/%d], Step [%d], Loss: %.4f, Perplexity: %5.4f' % (epoch, num_epochs, i_step, loss.item(), np.exp(loss.item()))\n",
    "\n",
    "            # Print training statistics (on same line).\n",
    "            print('\\r' + stats, end=\"\")\n",
    "\n",
    "\n",
    "            # Print training statistics to file.\n",
    "            f.write(stats + '\\n')\n",
    "            f.flush()\n",
    "\n",
    "            # Print training statistics (on different line).\n",
    "            if i_step % print_every == 0:\n",
    "                print('\\r' + stats)\n",
    "            i_step=i_step+1\n",
    "        train_losses.append(loss.item())\n",
    "    except RuntimeError:\n",
    "        print(captions,i_step)\n",
    "        pass\n",
    "    validation_loss=0\n",
    "    validation_iter=1\n",
    "    for images,captions in valid_dataset.load_data(batch_size):\n",
    "        images=torch.cat(images)\n",
    "        \n",
    "\n",
    "        # Move batch of images and captions to GPU if CUDA is available.\n",
    "        images = images.to(device)\n",
    "        captions = [caption.to(device) for caption in captions]\n",
    "        captions_padded=nn.utils.rnn.pad_sequence(captions,batch_first=True)\n",
    "        # Zero the gradients.\n",
    "        decoder.zero_grad()\n",
    "        encoder.zero_grad()\n",
    "        \n",
    "        # Pass the inputs through the CNN-RNN model.\n",
    "        features = encoder(images)\n",
    "        outputs = decoder(features, captions)\n",
    "\n",
    "        \n",
    "        # Calculate the batch loss.\n",
    "#         print(\"outputs.shape: \", outputs.shape)\n",
    "        loss = criterion(outputs.contiguous().view(-1, vocab_size), captions_padded.view(-1))\n",
    "        validation_loss=validation_loss+loss.item()\n",
    "        validation_iter=validation_iter+1\n",
    "        # Get training statistics.\n",
    "    validation_loss = validation_loss/validation_iter\n",
    "    validation_losses.append(validation_loss)\n",
    "    stats = 'Epoch [%d/%d], Step [%d], Validation Loss: %.4f, Perplexity: %5.4f\\n' % (epoch, num_epochs, validation_iter, validation_loss, np.exp(validation_loss))\n",
    "    if validation_loss<validation_loss_min:\n",
    "        print(\"\\nmodel improved!\")\n",
    "        torch.save(decoder.state_dict(), os.path.join('bengali_models', 'decoder.pkl'))\n",
    "        torch.save(encoder.state_dict(), os.path.join('bengali_models', 'encoder.pkl'))\n",
    "        validation_loss_min=validation_loss\n",
    "    else:\n",
    "        print(\"\\nnot improved yet!\")\n",
    "    # Print training statistics (on same line).\n",
    "    print('\\n' + stats, end=\"\")\n",
    "\n",
    "\n",
    "    # Print training statistics to file.\n",
    "    validation_f.write(stats + '\\n')\n",
    "    validation_f.flush()\n",
    "\n",
    "    # Print training statistics (on different line).\n",
    "\n",
    "\n",
    "\n",
    "# Close the training log file.\n",
    "f.close()\n",
    "validation_f.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "encoder.load_state_dict(torch.load(\"bengali_models/encoder.pkl\",map_location=\"cpu\"))\n",
    "decoder.load_state_dict(torch.load(\"bengali_models/decoder.pkl\",map_location=\"cpu\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.plot(train_losses,label=\"Train Loss\")\n",
    "plt.plot(validation_losses,label=\"Validation Loss\")\n",
    "plt.ylabel(\"Loss\")\n",
    "plt.xlabel(\"Epochs\")\n",
    "plt.title(\"Loss curve\")\n",
    "plt.legend()"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "validation_loss/"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import cv2\n",
    "\n",
    "work_image=cv2.imread(\"work.jpg\")\n",
    "\n",
    "work_image= cv2.resize(work_image,(224,224))\n",
    "\n",
    "work_image= cv2.cvtColor(work_image,cv2.COLOR_BGR2RGB)\n",
    "\n",
    "work_image=work_image/255\n",
    "\n",
    "work_image_tensor= torch.from_numpy(work_image)\n",
    "\n",
    "work_image_tensor.size()\n",
    "\n",
    "work_image_tensor=work_image_tensor.permute((2,0,1))"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "images_gpu = images.to(device)\n",
    "captions_gpu = [caption.to(device) for caption in captions]\n",
    "captions_padded=nn.utils.rnn.pad_sequence(captions_gpu,batch_first=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def sample(inputs):\n",
    "    \" accepts pre-processed image tensor (inputs) and returns predicted sentence (list of tensor ids of length max_len) \"\n",
    "\n",
    "\n",
    "    output = []\n",
    "    batch_size = inputs.shape[0] # batch_size is 1 at inference, inputs shape : (1, 1, embed_size)\n",
    "    hidden = decoder.init_hidden(batch_size) # Get initial hidden state of the LSTM\n",
    "\n",
    "    while True:\n",
    "        lstm_out, hidden = decoder.lstm(inputs, hidden) # lstm_out shape : (1, 1, hidden_size)\n",
    "        outputs = decoder.linear(lstm_out)  # outputs shape : (1, 1, vocab_size)\n",
    "        outputs= nn.functional.softmax(outputs,dim=2)\n",
    "        outputs = outputs.squeeze(1) # outputs shape : (1, vocab_size)\n",
    "        _, max_indice = torch.max(outputs, dim=1) # predict the most likely next word, max_indice shape : (1)\n",
    "\n",
    "        output.append(max_indice.cpu().numpy()[0].item()) # storing the word predicted\n",
    "\n",
    "        if (max_indice == 0):\n",
    "            # We predicted the <end> word, so there is no further prediction to do\n",
    "            break\n",
    "\n",
    "        ## Prepare to embed the last predicted word to be the new input of the lstm\n",
    "        inputs = decoder.word_embeddings(max_indice) # inputs shape : (1, embed_size)\n",
    "        inputs = inputs.unsqueeze(1) # inputs shape : (1, 1, embed_size)\n",
    "\n",
    "    return output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.font_manager as fm\n",
    "prop = fm.FontProperties(fname='Kalpurush.ttf')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "target_folder=\"flickr8k_images_outputs/test/\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "count=0\n",
    "for validation_image,validation_caption in test_dataset:\n",
    "    encoder.eval()\n",
    "    decoder.eval()\n",
    "    validation_image=validation_image.to(device)\n",
    "    validation_image=validation_image.view(1,3,224,224)\n",
    "    actual_outputs=validation_caption.cpu().numpy().tolist()\n",
    "    features=encoder(validation_image)\n",
    "    outputs=sample(features.view(1,1,4096))\n",
    "    output_words=[vocab.idx2word[output] for output in outputs]\n",
    "    plt.figure()\n",
    "    plt.imshow(validation_image[0].cpu().numpy().transpose((1,2,0)))\n",
    "    plt.title(\" \".join(output_words),fontproperties=prop)\n",
    "    plt.savefig(\"{}/{}.jpg\".format(target_folder,count))\n",
    "    count=count+1\n",
    "#     if count>100:\n",
    "#         break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "work_image_tensor.float().type()\n",
    "\n",
    "encoder.eval()\n",
    "decoder.eval()\n",
    "validation_image=work_image_tensor.to(device)\n",
    "validation_image=validation_image.view(1,3,224,224)\n",
    "features=encoder(validation_image.float())\n",
    "outputs=sample(features.view(1,1,4096))\n",
    "output_words=[vocab.idx2word[output] for output in outputs]\n",
    "plt.figure()\n",
    "plt.imshow(validation_image[0].cpu().numpy().transpose((1,2,0)))\n",
    "plt.title(\" \".join(output_words[:-1]),fontproperties=prop)\n",
    "plt.savefig(\"work_result.jpg\")#.format(target_folder,count))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for output in actual_outputs:\n",
    "    print(vocab.idx2word[output])"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "nn.functional.softmax()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
