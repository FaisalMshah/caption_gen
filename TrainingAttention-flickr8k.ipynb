{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import cv2\n",
    "import random\n",
    "import numpy as np\n",
    "import random"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "SEED=1234\n",
    "np.random.seed(SEED)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from torch.utils import data\n",
    "import torch.nn as nn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package punkt to /root/nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n"
     ]
    }
   ],
   "source": [
    "from dataloader import Dataset\n",
    "from vocabulary import Vocabulary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset_folder=\"/floyd/input/flickr8k/\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0/40460] Tokenizing captions...\n",
      "[1000/40460] Tokenizing captions...\n",
      "[2000/40460] Tokenizing captions...\n",
      "[3000/40460] Tokenizing captions...\n",
      "[4000/40460] Tokenizing captions...\n",
      "[5000/40460] Tokenizing captions...\n",
      "[6000/40460] Tokenizing captions...\n",
      "[7000/40460] Tokenizing captions...\n",
      "[8000/40460] Tokenizing captions...\n",
      "[9000/40460] Tokenizing captions...\n",
      "[10000/40460] Tokenizing captions...\n",
      "[11000/40460] Tokenizing captions...\n",
      "[12000/40460] Tokenizing captions...\n",
      "[13000/40460] Tokenizing captions...\n",
      "[14000/40460] Tokenizing captions...\n",
      "[15000/40460] Tokenizing captions...\n",
      "[16000/40460] Tokenizing captions...\n",
      "[17000/40460] Tokenizing captions...\n",
      "[18000/40460] Tokenizing captions...\n",
      "[19000/40460] Tokenizing captions...\n",
      "[20000/40460] Tokenizing captions...\n",
      "[21000/40460] Tokenizing captions...\n",
      "[22000/40460] Tokenizing captions...\n",
      "[23000/40460] Tokenizing captions...\n",
      "[24000/40460] Tokenizing captions...\n",
      "[25000/40460] Tokenizing captions...\n",
      "[26000/40460] Tokenizing captions...\n",
      "[27000/40460] Tokenizing captions...\n",
      "[28000/40460] Tokenizing captions...\n",
      "[29000/40460] Tokenizing captions...\n",
      "[30000/40460] Tokenizing captions...\n",
      "[31000/40460] Tokenizing captions...\n",
      "[32000/40460] Tokenizing captions...\n",
      "[33000/40460] Tokenizing captions...\n",
      "[34000/40460] Tokenizing captions...\n",
      "[35000/40460] Tokenizing captions...\n",
      "[36000/40460] Tokenizing captions...\n",
      "[37000/40460] Tokenizing captions...\n",
      "[38000/40460] Tokenizing captions...\n",
      "[39000/40460] Tokenizing captions...\n",
      "[40000/40460] Tokenizing captions...\n"
     ]
    }
   ],
   "source": [
    "tokens=open(dataset_folder+\"Flickr8k.token.txt\").read().split(\"\\n\")\n",
    "\n",
    "for token in tokens:\n",
    "    if token=='':\n",
    "        tokens.remove(token)\n",
    "\n",
    "train_images_text_file=dataset_folder+\"Flickr_8k.trainImages.txt\"\n",
    "validation_images_text_file=dataset_folder+\"Flickr_8k.devImages.txt\"\n",
    "test_images_text_file=dataset_folder+\"Flickr_8k.testImages.txt\"\n",
    "\n",
    "\n",
    "train_images_text=open(train_images_text_file,\"r\").read()\n",
    "validation_images_text=open(validation_images_text_file,\"r\").read()\n",
    "test_images_text=open(test_images_text_file,\"r\").read()\n",
    "\n",
    "train_images_names=train_images_text.split(\"\\n\")\n",
    "validation_images_names=validation_images_text.split(\"\\n\")\n",
    "test_images_names=test_images_text.split(\"\\n\")\n",
    "\n",
    "image_folder= dataset_folder+\"Flicker8k_Dataset/\"\n",
    "\n",
    "image_names=[]\n",
    "captions=[]\n",
    "train_with_captions=[]\n",
    "val_with_captions=[]\n",
    "test_with_captions=[]\n",
    "for token in tokens:\n",
    "    image_name=token.split(\"\\t\")[0].split(\"#\")[0]\n",
    "    caption= token.split(\"\\t\")[1]\n",
    "    image_names.append(image_name)\n",
    "    captions.append(caption)\n",
    "    if image_name in train_images_names:\n",
    "        train_with_captions.append((image_name,caption))\n",
    "    elif image_name in validation_images_names:\n",
    "        val_with_captions.append((image_name,caption))\n",
    "    elif image_name in test_images_names:\n",
    "        test_with_captions.append((image_name,caption))\n",
    "\n",
    "vocab=Vocabulary(vocab_threshold=6,captions=captions)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torchvision import transforms\n",
    "\n",
    "# Define a transform to pre-process the training images.\n",
    "transform_train = transforms.Compose([ \n",
    "    transforms.Resize(224),                           # get 224x224 crop from random location\n",
    "    transforms.RandomHorizontalFlip(),               # horizontally flip image with probability=0.5\n",
    "    transforms.ToTensor(),                           # convert the PIL Image to a tensor\n",
    "])\n",
    "transform_test = transforms.Compose([ \n",
    "    transforms.Resize(224),                          # smaller edge of image resized to 256\n",
    "    transforms.ToTensor(),                           # convert the PIL Image to a tensor\n",
    "])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset= Dataset(image_folder,train_with_captions,transform_train,vocab)\n",
    "valid_dataset= Dataset(image_folder,val_with_captions,transform_train,vocab)\n",
    "test_dataset= Dataset(image_folder,test_with_captions,transform_train,vocab)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "from model import EncoderCNN,EncoderVGG,EncoderVGGAtt\n",
    "from model import DecoderRNN,DecoderRNNAttention,Attention"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "batch_size = 32          # batch size\n",
    "vocab_threshold = 6        # minimum word count threshold\n",
    "vocab_from_file = True    # if True, load existing vocab file\n",
    "embed_size = 512           # dimensionality of image and word embeddings\n",
    "hidden_size = 512          # number of features in hidden state of the RNN decoder\n",
    "num_epochs = 2          # number of training epochs (1 for testing)\n",
    "save_every = 1             # determines frequency of saving model weights\n",
    "print_every = 200          # determines window for printing average loss\n",
    "log_file = 'training_log_attention_flickr8k.txt'       # name of file with saved training loss and perplexity\n",
    "val_log_file = 'validation_log_attention_flickr8k.txt'\n",
    "vocab_size=len(vocab)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "2656"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "vocab_size"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "encoder = EncoderVGGAtt()\n",
    "decoder = DecoderRNNAttention(embed_size, hidden_size, vocab_size,batch_size)\n",
    "\n",
    "# Move models to GPU if CUDA is available. \n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "encoder.to(device)\n",
    "decoder.to(device)\n",
    "\n",
    "# Define the loss function. \n",
    "criterion = nn.CrossEntropyLoss().cuda() if torch.cuda.is_available() else nn.CrossEntropyLoss()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "params = list(decoder.parameters())+list(decoder.attention.parameters())# + list(encoder.embed.parameters()) \n",
    "optimizer = torch.optim.Adam(params, lr=0.001,weight_decay=0.001)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "validation_loss_min = np.inf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_losses=[]\n",
    "validation_losses=[]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [1/2], Step [0], Loss: 7.8976, Perplexity: 2690.8389\n",
      "Epoch [1/2], Step [100], Loss: 5.7490, Perplexity: 313.8882\n",
      "Epoch [1/2], Step [200], Loss: 5.1998, Perplexity: 181.2373\n",
      "Epoch [1/2], Step [300], Loss: 5.3804, Perplexity: 217.0985\n",
      "Epoch [1/2], Step [400], Loss: 5.6252, Perplexity: 277.3408\n",
      "Epoch [1/2], Step [500], Loss: 5.5730, Perplexity: 263.2232\n",
      "Epoch [1/2], Step [600], Loss: 5.3152, Perplexity: 203.4013\n",
      "Epoch [1/2], Step [700], Loss: 5.3152, Perplexity: 203.4073\n",
      "Epoch [1/2], Step [800], Loss: 5.4660, Perplexity: 236.5161\n",
      "Epoch [1/2], Step [900], Loss: 5.3398, Perplexity: 208.4659\n",
      "Epoch [1/2], Step [936], Loss: 5.7002, Perplexity: 298.9236\n",
      "model improved!\n",
      "\n",
      "Epoch [1/2], Step [157], Validation Loss: 5.2964, Perplexity: 199.6180\n",
      "Epoch [2/2], Step [1000], Loss: 5.3634, Perplexity: 213.4591\n",
      "Epoch [2/2], Step [1100], Loss: 4.9645, Perplexity: 143.2418\n",
      "Epoch [2/2], Step [1200], Loss: 6.0592, Perplexity: 428.0228\n",
      "Epoch [2/2], Step [1300], Loss: 5.0687, Perplexity: 158.9608\n",
      "Epoch [2/2], Step [1400], Loss: 5.4954, Perplexity: 243.5774\n",
      "Epoch [2/2], Step [1500], Loss: 5.0777, Perplexity: 160.4008\n",
      "Epoch [2/2], Step [1600], Loss: 5.7440, Perplexity: 312.3231\n",
      "Epoch [2/2], Step [1700], Loss: 5.7545, Perplexity: 315.6074\n",
      "Epoch [2/2], Step [1800], Loss: 5.4133, Perplexity: 224.3774\n",
      "Epoch [2/2], Step [1873], Loss: 5.7244, Perplexity: 306.2553\n",
      "not improved yet!\n",
      "\n",
      "Epoch [2/2], Step [157], Validation Loss: 5.3042, Perplexity: 201.1832\n"
     ]
    }
   ],
   "source": [
    "f = open(log_file, 'w')\n",
    "validation_f=open(val_log_file,'w')\n",
    "i_step=0\n",
    "print_every=100\n",
    "# torch.autograd.set_detect_anomaly(True)\n",
    "for epoch in range(1, num_epochs+1):\n",
    "#     try:\n",
    "    if True:\n",
    "        for images,captions in dataset.load_data(batch_size):\n",
    "            images=torch.cat(images)\n",
    "\n",
    "\n",
    "            # Move batch of images and captions to GPU if CUDA is available.\n",
    "            images_gpu = images.to(device)\n",
    "            captions_gpu = [caption.to(device) for caption in captions]\n",
    "            target_captions = [caption[1:] for caption in captions_gpu]\n",
    "            captions_padded=nn.utils.rnn.pad_sequence(target_captions,batch_first=True)\n",
    "            # Zero the gradients.\n",
    "            decoder.zero_grad()\n",
    "            encoder.zero_grad()\n",
    "\n",
    "            # Pass the inputs through the CNN-RNN model.\n",
    "            features = encoder(images_gpu)\n",
    "            outputs,alphas = decoder(features, captions_gpu)\n",
    "\n",
    "            # Calculate the batch loss.\n",
    "    #         print(\"outputs.shape: \", outputs.shape)\n",
    "            loss = criterion(outputs.contiguous().view(-1, vocab_size), captions_padded.view(-1))\n",
    "            # Backward pass.\n",
    "            loss.backward()\n",
    "\n",
    "            # Update the parameters in the optimizer.\n",
    "            optimizer.step()\n",
    "\n",
    "            # Get training statistics.\n",
    "            stats = 'Epoch [%d/%d], Step [%d], Loss: %.4f, Perplexity: %5.4f' % (epoch, num_epochs, i_step, loss.item(), np.exp(loss.item()))\n",
    "\n",
    "            # Print training statistics (on same line).\n",
    "            print('\\r' + stats, end=\"\")\n",
    "\n",
    "\n",
    "            # Print training statistics to file.\n",
    "            f.write(stats + '\\n')\n",
    "            f.flush()\n",
    "\n",
    "            # Print training statistics (on different line).\n",
    "            if i_step % print_every == 0:\n",
    "                print('\\r' + stats)\n",
    "            i_step=i_step+1\n",
    "        train_losses.append(loss.item())\n",
    "#     except RuntimeError as e:\n",
    "#         print(e)\n",
    "#         break\n",
    "    validation_loss=0\n",
    "    validation_iter=1\n",
    "    for images,captions in valid_dataset.load_data(batch_size):\n",
    "        images=torch.cat(images)\n",
    "        \n",
    "\n",
    "        # Move batch of images and captions to GPU if CUDA is available.\n",
    "        images = images.to(device)\n",
    "        captions = [caption.to(device) for caption in captions]\n",
    "        target_captions=[caption[1:] for caption in captions]\n",
    "        captions_padded=nn.utils.rnn.pad_sequence(target_captions,batch_first=True)\n",
    "        # Zero the gradients.\n",
    "        decoder.zero_grad()\n",
    "        encoder.zero_grad()\n",
    "        \n",
    "        # Pass the inputs through the CNN-RNN model.\n",
    "        features = encoder(images)\n",
    "        outputs,alphas = decoder(features, captions)\n",
    "\n",
    "        \n",
    "        # Calculate the batch loss.\n",
    "#         print(\"outputs.shape: \", outputs.shape)\n",
    "        loss = criterion(outputs.contiguous().view(-1, vocab_size), captions_padded.view(-1))\n",
    "        validation_loss=validation_loss+loss.item()\n",
    "        validation_iter=validation_iter+1\n",
    "        # Get training statistics.\n",
    "    validation_loss = validation_loss/validation_iter\n",
    "    validation_losses.append(validation_loss)\n",
    "    stats = 'Epoch [%d/%d], Step [%d], Validation Loss: %.4f, Perplexity: %5.4f\\n' % (epoch, num_epochs, validation_iter, validation_loss, np.exp(validation_loss))\n",
    "    if validation_loss<validation_loss_min:\n",
    "        print(\"\\nmodel improved!\")\n",
    "        torch.save(decoder.state_dict(), os.path.join('flickr8k_models_attention', 'decoder.pkl'))\n",
    "        torch.save(encoder.state_dict(), os.path.join('flickr8k_models_attention', 'encoder.pkl'))\n",
    "        validation_loss_min=validation_loss\n",
    "    else:\n",
    "        print(\"\\nnot improved yet!\")\n",
    "    # Print training statistics (on same line).\n",
    "    print('\\n' + stats, end=\"\")\n",
    "\n",
    "\n",
    "    # Print training statistics to file.\n",
    "    validation_f.write(stats + '\\n')\n",
    "    validation_f.flush()\n",
    "\n",
    "    # Print training statistics (on different line).\n",
    "\n",
    "\n",
    "\n",
    "# Close the training log file.\n",
    "f.close()\n",
    "validation_f.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "IncompatibleKeys(missing_keys=[], unexpected_keys=[])"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "encoder.load_state_dict(torch.load(\"flickr8k_models_attention/encoder.pkl\"))#,map_location=\"cpu\"))\n",
    "decoder.load_state_dict(torch.load(\"flickr8k_models_attention/decoder.pkl\"))#,map_location=\"cpu\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "plt.plot(train_losses,label=\"Train Loss\")\n",
    "plt.plot(validation_losses,label=\"Validation Loss\")\n",
    "plt.ylabel(\"Loss\")\n",
    "plt.xlabel(\"Epochs\")\n",
    "plt.title(\"Loss curve\")\n",
    "plt.legend()"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "validation_loss/"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "images_gpu = images.to(device)\n",
    "captions_gpu = [caption.to(device) for caption in captions]\n",
    "captions_padded=nn.utils.rnn.pad_sequence(captions_gpu,batch_first=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "from torch import nn\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "5000"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(valid_dataset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'validation_image' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-23-3dbd8648f6a7>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mvalidation_image\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdevice\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m: name 'validation_image' is not defined"
     ]
    }
   ],
   "source": [
    "validation_image.device"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "ename": "RuntimeError",
     "evalue": "input has inconsistent input_size: got 49, expected 512",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-25-de6b8fdfb22b>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     31\u001b[0m         \u001b[0mhidden\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mdecoder\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mhidden\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     32\u001b[0m         \u001b[0mcell\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mdecoder\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcell\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 33\u001b[0;31m         \u001b[0mhidden\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mcell\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mdecoder\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdecode_step\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0membedding\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mhidden\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mcell\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     34\u001b[0m         \u001b[0mdecoder\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mhidden\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mdecoder\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcell\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mhidden\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mcell\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     35\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.6/site-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m    491\u001b[0m             \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_slow_forward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    492\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 493\u001b[0;31m             \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mforward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    494\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0mhook\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_forward_hooks\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mvalues\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    495\u001b[0m             \u001b[0mhook_result\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mhook\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mresult\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.6/site-packages/torch/nn/modules/rnn.py\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, input, hx)\u001b[0m\n\u001b[1;32m    879\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mforward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mhx\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mNone\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    880\u001b[0m         \u001b[0;31m# type: (Tensor, Optional[Tuple[Tensor, Tensor]]) -> Tuple[Tensor, Tensor]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 881\u001b[0;31m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcheck_forward_input\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    882\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mhx\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    883\u001b[0m             \u001b[0mzeros\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mzeros\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msize\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mhidden_size\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdtype\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdtype\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdevice\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdevice\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.6/site-packages/torch/nn/modules/rnn.py\u001b[0m in \u001b[0;36mcheck_forward_input\u001b[0;34m(self, input)\u001b[0m\n\u001b[1;32m    700\u001b[0m             raise RuntimeError(\n\u001b[1;32m    701\u001b[0m                 \"input has inconsistent input_size: got {}, expected {}\".format(\n\u001b[0;32m--> 702\u001b[0;31m                     input.size(1), self.input_size))\n\u001b[0m\u001b[1;32m    703\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    704\u001b[0m     \u001b[0;34m@\u001b[0m\u001b[0mweak_script_method\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mRuntimeError\u001b[0m: input has inconsistent input_size: got 49, expected 512"
     ]
    }
   ],
   "source": [
    "encoder.eval()\n",
    "decoder.eval()\n",
    "count=0\n",
    "for validation_image,validation_caption in valid_dataset:\n",
    "    validation_image=validation_image.to(device)\n",
    "    validation_caption=validation_caption.to(device)\n",
    "    features=encoder(validation_image.unsqueeze(0))\n",
    "    start =0\n",
    "    start_tensor=torch.tensor(start).to(device)\n",
    "    start_tensor.to(\"cpu\")\n",
    "    word_embedding= decoder.word_embeddings(start_tensor)\n",
    "    word_embedding.size()\n",
    "    batch_size=features.shape[0]\n",
    "    word_alphas=[]\n",
    "    words=[]\n",
    "\n",
    "    decoder.hidden,decoder.cell=decoder.init_h(features).unsqueeze(0),decoder.init_c(features).unsqueeze(0)\n",
    "    \n",
    "\n",
    "    while True:\n",
    "        attention_feature,alpha = decoder.attention(features,decoder.hidden)\n",
    "\n",
    "        word_alphas.append(alpha)\n",
    "\n",
    "#         gate= decoder.sigmoid(decoder.f_beta(decoder.hidden))\n",
    "\n",
    "#         attention_feature=attention_feature*gate\n",
    "\n",
    "        embedding= attention_feature+word_embedding.unsqueeze(0)\n",
    "\n",
    "        hidden=decoder.hidden\n",
    "        cell=decoder.cell\n",
    "        hidden,cell=decoder.decode_step(embedding,(hidden,cell))\n",
    "        decoder.hidden,decoder.cell=hidden,cell\n",
    "\n",
    "        pred_word= decoder.linear(decoder.hidden)\n",
    "\n",
    "        pred_word = nn.functional.softmax(pred_word)\n",
    "\n",
    "        word_indice=torch.max(pred_word,dim=1)[1]\n",
    "        words.append(vocab.idx2word[word_indice.item()])\n",
    "        word_embedding = decoder.word_embeddings(word_indice[0])\n",
    "        if word_indice==1:\n",
    "            break\n",
    "    plt.figure() \n",
    "    plt.imshow(validation_image.cpu().numpy().transpose((1,2,0)))\n",
    "    plt.title(\" \".join(words))\n",
    "    count=count+1\n",
    "    if count>99:\n",
    "        break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([1, 1, 49, 512])"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "decoder.hidden.size()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\"\"\" Define the feattention_feature behavior of the model \"\"\"\n",
    "input_captions=[[self.word_embeddings(word) for word in single_caption[:-1]] for single_caption in captions]\n",
    "pred_captions=[]\n",
    "caption_alphas=[]\n",
    "\n",
    "#         input_captions=nn.utils.rnn.pad_sequence(input_captions,batch_first=True)\n",
    "\n",
    "#         input_captions_lengths=[len(input_caption)+1 for input_caption in input_captions]\n",
    "#         embeddings = self.word_embeddings(input_captions) # embeddings new shape : (batch_size, captions length - 1, embed_size)\n",
    "\n",
    "batch_size = features.shape[0] # features is of shape (batch_size, embed_size)\n",
    "for i,input_caption in enumerate(input_captions):\n",
    "    self.hidden,self.cell=self.init_hidden(1)\n",
    "    feature=features[i]\n",
    "    #print(\"feature size\")\n",
    "    #print(feature.size())\n",
    "\n",
    "    #print(self.hidden.size())\n",
    "    #print(self.cell.size())\n",
    "    pred_words=[]\n",
    "    word_alphas=[]\n",
    "    for input_word in input_caption:\n",
    "        attention_feature,alpha= self.attention(feature.view(1,feature.size(0),feature.size(1)),self.hidden)\n",
    "        word_alphas.append(alpha)\n",
    "        gate=self.sigmoid(self.f_beta(self.hidden))\n",
    "        #print(\"attention feature\")\n",
    "        #print(attention_feature.size())\n",
    "        #print(\"gate \")\n",
    "        #print(gate.size())\n",
    "        attention_feature=attention_feature*gate\n",
    "        #print(attention_feature.size())\n",
    "        #print(input_word.size())\n",
    "        embedding= torch.cat((attention_feature,input_word.unsqueeze(0)),dim=1)\n",
    "        hidden=self.hidden\n",
    "        cell=self.cell\n",
    "        hidden,cell=self.decode_step(embedding,(hidden,cell))\n",
    "        #print(\"hidden and cell\")\n",
    "        #print(hidden.size())\n",
    "        #print(cell.size())\n",
    "        self.hidden,self.cell =hidden,cell\n",
    "        pred_word=self.linear(self.dropout(self.hidden))\n",
    "        pred_words.append(pred_word)\n",
    "    pred_captions.append(torch.cat(pred_words))\n",
    "    caption_alphas.append(word_alphas)\n",
    "pred_captions_padded=nn.utils.rnn.pad_sequence(pred_captions,batch_first=True)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def sample(features):\n",
    "    \" accepts pre-processed image tensor (features) and returns predicted sentence (list of tensor ids of length max_len) \"\n",
    "\n",
    "\n",
    "    output = []\n",
    "    batch_size = features.shape[0] # batch_size is 1 at inference, features shape : (1, 1, embed_size)\n",
    "    hidden = decoder.init_hidden(batch_size) # Get initial hidden state of the LSTM\n",
    "\n",
    "    while True:\n",
    "        lstm_out, hidden = decoder.lstm(features, hidden) # lstm_out shape : (1, 1, hidden_size)\n",
    "        outputs = decoder.linear(lstm_out)  # outputs shape : (1, 1, vocab_size)\n",
    "        outputs= nn.functional.softmax(outputs,dim=2)\n",
    "        outputs = outputs.squeeze(1) # outputs shape : (1, vocab_size)\n",
    "        _, max_indice = torch.max(outputs, dim=1) # predict the most likely next word, max_indice shape : (1)\n",
    "\n",
    "        output.append(max_indice.cpu().numpy()[0].item()) # storing the word predicted\n",
    "\n",
    "        if (max_indice == 0):\n",
    "            # We predicted the <end> word, so there is no further prediction to do\n",
    "            break\n",
    "\n",
    "        ## Prepare to embed the last predicted word to be the new input of the lstm\n",
    "        features = decoder.word_embeddings(max_indice) # features shape : (1, embed_size)\n",
    "        features = features.unsqueeze(1) # features shape : (1, 1, embed_size)\n",
    "\n",
    "    return output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "outputs=sample(features.view(1,1,4096))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for output in outputs:\n",
    "    print(vocab.idx2word[output])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for output in actual_outputs:\n",
    "    print(vocab.idx2word[output])"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "nn.functional.softmax()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
