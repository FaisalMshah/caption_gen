{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import cv2\n",
    "import random\n",
    "import numpy as np\n",
    "import random"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "SEED=1234\n",
    "np.random.seed(SEED)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from torch.utils import data\n",
    "import torch.nn as nn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package punkt to /root/nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n"
     ]
    }
   ],
   "source": [
    "from dataloader import Dataset\n",
    "from vocabulary import Vocabulary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset_folder=\"/floyd/input/bangla_image_caption/\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: bnltk in /usr/local/lib/python3.6/site-packages (0.7.6)\n",
      "Requirement already satisfied: keras in /usr/local/lib/python3.6/site-packages (from bnltk) (2.2.4)\n",
      "Requirement already satisfied: requests in /usr/local/lib/python3.6/site-packages (from bnltk) (2.22.0)\n",
      "Requirement already satisfied: sklearn in /usr/local/lib/python3.6/site-packages (from bnltk) (0.0)\n",
      "Requirement already satisfied: tensorflow in /usr/local/lib/python3.6/site-packages (from bnltk) (1.14.0)\n",
      "Requirement already satisfied: numpy in /usr/local/lib/python3.6/site-packages (from bnltk) (1.15.1)\n",
      "Requirement already satisfied: scipy>=0.14 in /usr/local/lib/python3.6/site-packages (from keras->bnltk) (1.1.0)\n",
      "Requirement already satisfied: six>=1.9.0 in /usr/local/lib/python3.6/site-packages (from keras->bnltk) (1.11.0)\n",
      "Requirement already satisfied: pyyaml in /usr/local/lib/python3.6/site-packages (from keras->bnltk) (3.13)\n",
      "Requirement already satisfied: h5py in /usr/local/lib/python3.6/site-packages (from keras->bnltk) (2.8.0)\n",
      "Requirement already satisfied: keras_applications>=1.0.6 in /usr/local/lib/python3.6/site-packages (from keras->bnltk) (1.0.8)\n",
      "Requirement already satisfied: keras_preprocessing>=1.0.5 in /usr/local/lib/python3.6/site-packages (from keras->bnltk) (1.1.0)\n",
      "Requirement already satisfied: urllib3!=1.25.0,!=1.25.1,<1.26,>=1.21.1 in /usr/local/lib/python3.6/site-packages (from requests->bnltk) (1.24.3)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.6/site-packages (from requests->bnltk) (2019.6.16)\n",
      "Requirement already satisfied: idna<2.9,>=2.5 in /usr/local/lib/python3.6/site-packages (from requests->bnltk) (2.8)\n",
      "Requirement already satisfied: chardet<3.1.0,>=3.0.2 in /usr/local/lib/python3.6/site-packages (from requests->bnltk) (3.0.4)\n",
      "Requirement already satisfied: scikit-learn in /usr/local/lib/python3.6/site-packages (from sklearn->bnltk) (0.19.2)\n",
      "Requirement already satisfied: gast>=0.2.0 in /usr/local/lib/python3.6/site-packages (from tensorflow->bnltk) (0.2.2)\n",
      "Requirement already satisfied: astor>=0.6.0 in /usr/local/lib/python3.6/site-packages (from tensorflow->bnltk) (0.8.0)\n",
      "Requirement already satisfied: grpcio>=1.8.6 in /usr/local/lib/python3.6/site-packages (from tensorflow->bnltk) (1.23.0)\n",
      "Requirement already satisfied: tensorflow-estimator<1.15.0rc0,>=1.14.0rc0 in /usr/local/lib/python3.6/site-packages (from tensorflow->bnltk) (1.14.0)\n",
      "Requirement already satisfied: google-pasta>=0.1.6 in /usr/local/lib/python3.6/site-packages (from tensorflow->bnltk) (0.1.7)\n",
      "Requirement already satisfied: protobuf>=3.6.1 in /usr/local/lib/python3.6/site-packages (from tensorflow->bnltk) (3.9.1)\n",
      "Requirement already satisfied: tensorboard<1.15.0,>=1.14.0 in /usr/local/lib/python3.6/site-packages (from tensorflow->bnltk) (1.14.0)\n",
      "Requirement already satisfied: wheel>=0.26 in /usr/local/lib/python3.6/site-packages (from tensorflow->bnltk) (0.31.1)\n",
      "Requirement already satisfied: absl-py>=0.7.0 in /usr/local/lib/python3.6/site-packages (from tensorflow->bnltk) (0.8.0)\n",
      "Requirement already satisfied: wrapt>=1.11.1 in /usr/local/lib/python3.6/site-packages (from tensorflow->bnltk) (1.11.2)\n",
      "Requirement already satisfied: termcolor>=1.1.0 in /usr/local/lib/python3.6/site-packages (from tensorflow->bnltk) (1.1.0)\n",
      "Requirement already satisfied: setuptools in /usr/local/lib/python3.6/site-packages (from protobuf>=3.6.1->tensorflow->bnltk) (46.4.0)\n",
      "Requirement already satisfied: werkzeug>=0.11.15 in /usr/local/lib/python3.6/site-packages (from tensorboard<1.15.0,>=1.14.0->tensorflow->bnltk) (0.15.5)\n",
      "Requirement already satisfied: markdown>=2.6.8 in /usr/local/lib/python3.6/site-packages (from tensorboard<1.15.0,>=1.14.0->tensorflow->bnltk) (3.1.1)\n",
      "\u001b[31mscikit-umfpack 0.3.2 has requirement numpy>=1.15.3, but you'll have numpy 1.15.1 which is incompatible.\u001b[0m\n",
      "\u001b[31mmenpo 0.9.2 has requirement matplotlib>=3.0, but you'll have matplotlib 2.2.3 which is incompatible.\u001b[0m\n",
      "\u001b[33mYou are using pip version 10.0.1, however version 20.2b1 is available.\n",
      "You should consider upgrading via the 'pip install --upgrade pip' command.\u001b[0m\n"
     ]
    }
   ],
   "source": [
    "!pip install bnltk"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset_folder=\"/floyd/input/bangla_image_caption/\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "caption_json_path=dataset_folder+\"captions.json\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "filenames_with_captions=json.load(open(caption_json_path))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "from bnltk.tokenize import Tokenizers\n",
    "t = Tokenizers()\n",
    "tokenizer = t.bn_word_tokenizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "train,test=train_test_split(filenames_with_captions,test_size=0.1)\n",
    "train,valid=train_test_split(filenames_with_captions,test_size=0.1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "image_names=[]\n",
    "all_captions=[]\n",
    "train_with_captions=[]\n",
    "val_with_captions=[]\n",
    "test_with_captions=[]\n",
    "image_folder= dataset_folder+\"images/\"\n",
    "for filename_caption in train:\n",
    "    image_name = filename_caption[\"filename\"]\n",
    "    captions=filename_caption[\"caption\"]\n",
    "    for caption in captions:\n",
    "        train_with_captions.append((image_name,caption))\n",
    "        all_captions.append(caption)\n",
    "for filename_caption in valid:\n",
    "    image_name = filename_caption[\"filename\"]\n",
    "    captions=filename_caption[\"caption\"]\n",
    "    for caption in captions:\n",
    "        val_with_captions.append((image_name,caption))\n",
    "        all_captions.append(caption)\n",
    "for filename_caption in test:\n",
    "    image_name = filename_caption[\"filename\"]\n",
    "    captions=filename_caption[\"caption\"]\n",
    "    for caption in captions:\n",
    "        test_with_captions.append((image_name,caption))\n",
    "        all_captions.append(caption)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0/20140] Tokenizing captions...\n",
      "[1000/20140] Tokenizing captions...\n",
      "[2000/20140] Tokenizing captions...\n",
      "[3000/20140] Tokenizing captions...\n",
      "[4000/20140] Tokenizing captions...\n",
      "[5000/20140] Tokenizing captions...\n",
      "[6000/20140] Tokenizing captions...\n",
      "[7000/20140] Tokenizing captions...\n",
      "[8000/20140] Tokenizing captions...\n",
      "[9000/20140] Tokenizing captions...\n",
      "[10000/20140] Tokenizing captions...\n",
      "[11000/20140] Tokenizing captions...\n",
      "[12000/20140] Tokenizing captions...\n",
      "[13000/20140] Tokenizing captions...\n",
      "[14000/20140] Tokenizing captions...\n",
      "[15000/20140] Tokenizing captions...\n",
      "[16000/20140] Tokenizing captions...\n",
      "[17000/20140] Tokenizing captions...\n",
      "[18000/20140] Tokenizing captions...\n",
      "[19000/20140] Tokenizing captions...\n",
      "[20000/20140] Tokenizing captions...\n"
     ]
    }
   ],
   "source": [
    "vocab=Vocabulary(vocab_threshold=6,captions=all_captions,tokenizer=t.bn_word_tokenizer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torchvision import transforms\n",
    "\n",
    "# Define a transform to pre-process the training images.\n",
    "transform_train = transforms.Compose([ \n",
    "    transforms.Resize(224),                           # get 224x224 crop from random location\n",
    "    transforms.RandomHorizontalFlip(),               # horizontally flip image with probability=0.5\n",
    "    transforms.ToTensor(),                           # convert the PIL Image to a tensor\n",
    "])\n",
    "transform_test = transforms.Compose([ \n",
    "    transforms.Resize(224),                          # smaller edge of image resized to 256\n",
    "    transforms.ToTensor(),                           # convert the PIL Image to a tensor\n",
    "])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset= Dataset(image_folder,train_with_captions,transform_train,vocab,tokenizer=t.bn_word_tokenizer)\n",
    "valid_dataset= Dataset(image_folder,val_with_captions,transform_test,vocab,tokenizer=t.bn_word_tokenizer)\n",
    "test_dataset= Dataset(image_folder,test_with_captions,transform_test,vocab,tokenizer=t.bn_word_tokenizer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "from model import EncoderCNN,EncoderVGG,EncoderVGGAtt\n",
    "from model import DecoderRNN,DecoderRNNAttention,Attention"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "batch_size = 128          # batch size\n",
    "vocab_threshold = 6        # minimum word count threshold\n",
    "vocab_from_file = True    # if True, load existing vocab file\n",
    "embed_size = 512           # dimensionality of image and word embeddings\n",
    "hidden_size = 512          # number of features in hidden state of the RNN decoder\n",
    "num_epochs = 20             # number of training epochs (1 for testing)\n",
    "save_every = 1             # determines frequency of saving model weights\n",
    "print_every = 200          # determines window for printing average loss\n",
    "log_file = 'training_log_attention.txt'       # name of file with saved training loss and perplexity\n",
    "val_log_file = 'validation_log_attention.txt'\n",
    "vocab_size=len(vocab)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1448"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "vocab_size"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "encoder = EncoderVGGAtt()\n",
    "decoder = DecoderRNNAttention(embed_size, hidden_size, vocab_size,batch_size,dropout=0.2)\n",
    "\n",
    "# Move models to GPU if CUDA is available. \n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "encoder.to(device)\n",
    "decoder.to(device)\n",
    "\n",
    "# Define the loss function. \n",
    "criterion = nn.CrossEntropyLoss().cuda() if torch.cuda.is_available() else nn.CrossEntropyLoss()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "params = list(decoder.parameters())# + list(encoder.embed.parameters()) \n",
    "optimizer = torch.optim.Adam(params, lr=0.001,weight_decay=0.001)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "validation_loss_min = np.inf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_losses=[]\n",
    "validation_losses=[]"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "class Attention(nn.Module):\n",
    "    \"\"\"\n",
    "    Attention Network.\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, encoder_dim, decoder_dim, attention_dim):\n",
    "        \"\"\"\n",
    "        :param encoder_dim: feature size of encoded images\n",
    "        :param decoder_dim: size of decoder's RNN\n",
    "        :param attention_dim: size of the attention network\n",
    "        \"\"\"\n",
    "        super(Attention, self).__init__()\n",
    "        self.encoder_att = nn.Linear(encoder_dim, attention_dim)  # linear layer to transform encoded image\n",
    "        self.decoder_att = nn.Linear(decoder_dim, attention_dim)  # linear layer to transform decoder's output\n",
    "        self.full_att = nn.Linear(attention_dim, 1)  # linear layer to calculate values to be softmax-ed\n",
    "        self.relu = nn.ReLU()\n",
    "        self.softmax = nn.Softmax(dim=1)  # softmax layer to calculate weights\n",
    "\n",
    "    def forward(self, encoder_out, decoder_hidden):\n",
    "        \"\"\"\n",
    "        Forward propagation.\n",
    "\n",
    "        :param encoder_out: encoded images, a tensor of dimension (batch_size, num_pixels, encoder_dim)\n",
    "        :param decoder_hidden: previous decoder output, a tensor of dimension (batch_size, decoder_dim)\n",
    "        :return: attention weighted encoding, weights\n",
    "        \"\"\"\n",
    "        att1 = self.encoder_att(encoder_out)  # (batch_size, num_pixels, attention_dim)\n",
    "        att2 = self.decoder_att(decoder_hidden)  # (batch_size, attention_dim)\n",
    "#         return att1,att2\n",
    "#         print(att2.unsqueeze(0).size())\n",
    "        att = self.full_att(self.relu(att1 + att2.unsqueeze(0))).squeeze(1)  # (batch_size, num_pixels)\n",
    "        alpha = self.softmax(att.unsqueeze(1))  # (batch_size, num_pixels)\n",
    "#         print(encoder_out.size())\n",
    "#         print(alpha.size())\n",
    "        attention_weighted_encoding = (encoder_out * alpha).sum(dim=0)  # (batch_size, encoder_dim)\n",
    "\n",
    "        return attention_weighted_encoding, alpha"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "class DecoderRNNAttention(nn.Module):\n",
    "    def __init__(self, embed_size, hidden_size, vocab_size,batch_size,dropout=0.5):\n",
    "        ''' Initialize the layers of this model.'''\n",
    "        super().__init__()\n",
    "\n",
    "        self.hidden_size = hidden_size\n",
    "\n",
    "        self.word_embeddings = nn.Embedding(vocab_size, embed_size)\n",
    "        \n",
    "\n",
    "        self.lstm = nn.LSTM(input_size=embed_size, \\\n",
    "                            hidden_size=hidden_size, # LSTM hidden units \n",
    "                            num_layers=1, # number of LSTM layer\n",
    "                            bias=True, # use bias weights b_ih and b_hh\n",
    "                            batch_first=True,  # input & output will have batch size as 1st dimension\n",
    "                            dropout=0, # Not applying dropout \n",
    "                            bidirectional=False, # unidirectional LSTM\n",
    "                       )\n",
    "        self.decode_step= nn.LSTMCell(input_size=(embed_size+hidden_size),hidden_size=hidden_size)\n",
    "        self.attention=Attention(embed_size,hidden_size,embed_size)\n",
    "        self.dropout= nn.Dropout(dropout)\n",
    "\n",
    "        self.linear = nn.Linear(hidden_size, vocab_size)                     \n",
    "        self.batch_size=batch_size\n",
    "        self.f_beta = nn.Linear(hidden_size, embed_size)  # linear layer to create a sigmoid-activated gate\n",
    "        self.sigmoid = nn.Sigmoid()\n",
    "        \n",
    "    def init_hidden(self, batch_size):\n",
    "        \"\"\" At the start of training, we need to initialize a hidden state;\n",
    "        there will be none because the hidden state is formed based on previously seen data.\n",
    "        So, this function defines a hidden state with all zeroes\n",
    "        The axes semantics are (num_layers, batch_size, hidden_dim)\n",
    "        \"\"\"\n",
    "        h=torch.zeros((batch_size, self.hidden_size), device=device)\n",
    "        c=torch.zeros((batch_size, self.hidden_size), device=device)\n",
    "        return h,c\n",
    "\n",
    "    def forward(self, features, captions):\n",
    "        \"\"\" Define the feedforward behavior of the model \"\"\"\n",
    "        input_captions=[[self.word_embeddings(word) for word in single_caption[:-1]] for single_caption in captions]\n",
    "        pred_captions=[]\n",
    "        caption_alphas=[]\n",
    "\n",
    "#         input_captions=nn.utils.rnn.pad_sequence(input_captions,batch_first=True)\n",
    "\n",
    "#         input_captions_lengths=[len(input_caption)+1 for input_caption in input_captions]\n",
    "#         embeddings = self.word_embeddings(input_captions) # embeddings new shape : (batch_size, captions length - 1, embed_size)\n",
    "\n",
    "        batch_size = features.shape[0] # features is of shape (batch_size, embed_size)\n",
    "        for i,input_caption in enumerate(input_captions):\n",
    "            self.hidden,self.cell=self.init_hidden(batch_size)\n",
    "            feature=features[i]\n",
    "#             print(feature.size())\n",
    "            pred_words=[]\n",
    "            word_alphas=[]\n",
    "            for input_word in input_caption:\n",
    "                attention_feature,alpha= self.attention(feature,self.hidden[i])\n",
    "                word_alphas.append(alpha)\n",
    "                gate=self.sigmoid(self.f_beta(self.hidden[i]))\n",
    "#                 print(attention_feature.size())\n",
    "#                 print(gate.size())\n",
    "                attention_feature=attention_feature*gate.unsqueeze(0)\n",
    "#                 print(attention_feature.size())\n",
    "#                 print(input_word.size())\n",
    "                embedding= torch.cat((attention_feature,input_word.unsqueeze(0)),dim=1)\n",
    "                hidden,cell=self.decode_step(embedding,(self.hidden[i].unsqueeze(0),self.cell[i].unsqueeze(0)))\n",
    "#                 print(hidden.size())\n",
    "#                 print(cell.size())\n",
    "                self.hidden[i],self.cell[i] =hidden.squeeze(0),cell.squeeze(0)\n",
    "                pred_word=self.linear(self.dropout(self.hidden[i]))\n",
    "                pred_words.append(pred_word)\n",
    "            pred_captions.append(torch.cat(pred_words))\n",
    "            caption_alphas.append(word_alphas)\n",
    "        pred_captions_padded=nn.utils.rnn.pad_sequence(pred_captions,batch_first=True)\n",
    "\n",
    "        return pred_captions_padded,caption_alphas"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "f = open(log_file, 'w')\n",
    "validation_f=open(val_log_file,'w')\n",
    "i_step=0\n",
    "print_every=100\n",
    "# torch.autograd.set_detect_anomaly(True)\n",
    "for epoch in range(1, num_epochs+1):\n",
    "    try:\n",
    "        for images,captions in dataset.load_data(batch_size):\n",
    "            images=torch.cat(images)\n",
    "\n",
    "\n",
    "            # Move batch of images and captions to GPU if CUDA is available.\n",
    "            images_gpu = images.to(device)\n",
    "            captions_gpu = [caption.to(device) for caption in captions]\n",
    "            target_captions = [caption[1:] for caption in captions_gpu]\n",
    "            captions_padded=nn.utils.rnn.pad_sequence(target_captions,batch_first=True)\n",
    "            # Zero the gradients.\n",
    "            decoder.zero_grad()\n",
    "            encoder.eval()\n",
    "\n",
    "            # Pass the inputs through the CNN-RNN model.\n",
    "            features = encoder(images_gpu)\n",
    "            outputs,alphas = decoder(features, captions_gpu)\n",
    "\n",
    "            # Calculate the batch loss.\n",
    "    #         print(\"outputs.shape: \", outputs.shape)\n",
    "            loss = criterion(outputs.contiguous().view(-1, vocab_size), captions_padded.view(-1))\n",
    "            # Backward pass.\n",
    "            loss.backward()\n",
    "\n",
    "            # Update the parameters in the optimizer.\n",
    "            optimizer.step()\n",
    "\n",
    "            # Get training statistics.\n",
    "            stats = 'Epoch [%d/%d], Step [%d], Loss: %.4f, Perplexity: %5.4f' % (epoch, num_epochs, i_step, loss.item(), np.exp(loss.item()))\n",
    "\n",
    "            # Print training statistics (on same line).\n",
    "            print('\\r' + stats, end=\"\")\n",
    "\n",
    "\n",
    "            # Print training statistics to file.\n",
    "            f.write(stats + '\\n')\n",
    "            f.flush()\n",
    "\n",
    "            # Print training statistics (on different line).\n",
    "            if i_step % print_every == 0:\n",
    "                print('\\r' + stats)\n",
    "            i_step=i_step+1\n",
    "        train_losses.append(loss.item())\n",
    "    except RuntimeError as e:\n",
    "        print(e)\n",
    "        break\n",
    "    validation_loss=0\n",
    "    validation_iter=1\n",
    "    for images,captions in valid_dataset.load_data(batch_size):\n",
    "        images=torch.cat(images)\n",
    "        \n",
    "\n",
    "        # Move batch of images and captions to GPU if CUDA is available.\n",
    "        images = images.to(device)\n",
    "        captions = [caption.to(device) for caption in captions]\n",
    "        target_captions=[caption[1:] for caption in captions]\n",
    "        captions_padded=nn.utils.rnn.pad_sequence(target_captions,batch_first=True)\n",
    "        # Zero the gradients.\n",
    "        decoder.zero_grad()\n",
    "        encoder.zero_grad()\n",
    "        \n",
    "        # Pass the inputs through the CNN-RNN model.\n",
    "        features = encoder(images)\n",
    "        outputs,alphas = decoder(features, captions)\n",
    "\n",
    "        \n",
    "        # Calculate the batch loss.\n",
    "#         print(\"outputs.shape: \", outputs.shape)\n",
    "        loss = criterion(outputs.contiguous().view(-1, vocab_size), captions_padded.view(-1))\n",
    "        validation_loss=validation_loss+loss.item()\n",
    "        validation_iter=validation_iter+1\n",
    "        # Get training statistics.\n",
    "    validation_loss = validation_loss/validation_iter\n",
    "    validation_losses.append(validation_loss)\n",
    "    stats = 'Epoch [%d/%d], Step [%d], Validation Loss: %.4f, Perplexity: %5.4f\\n' % (epoch, num_epochs, validation_iter, validation_loss, np.exp(validation_loss))\n",
    "    if validation_loss<validation_loss_min:\n",
    "        print(\"\\nmodel improved!\")\n",
    "        torch.save(decoder.state_dict(), os.path.join('bengali_models_attention', 'decoder.pkl'))\n",
    "        torch.save(encoder.state_dict(), os.path.join('bengali_models_attention', 'encoder.pkl'))\n",
    "        validation_loss_min=validation_loss\n",
    "    else:\n",
    "        print(\"\\nnot improved yet!\")\n",
    "    # Print training statistics (on same line).\n",
    "    print('\\n' + stats, end=\"\")\n",
    "\n",
    "\n",
    "    # Print training statistics to file.\n",
    "    validation_f.write(stats + '\\n')\n",
    "    validation_f.flush()\n",
    "\n",
    "    # Print training statistics (on different line).\n",
    "\n",
    "\n",
    "\n",
    "# Close the training log file.\n",
    "f.close()\n",
    "validation_f.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "encoder.load_state_dict(torch.load(\"bengali_models_attention/encoder.pkl\",map_location=\"cpu\"))\n",
    "decoder.load_state_dict(torch.load(\"bengali_models_attention/decoder.pkl\",map_location=\"cpu\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "plt.plot(train_losses,label=\"Train Loss\")\n",
    "plt.plot(validation_losses,label=\"Validation Loss\")\n",
    "plt.ylabel(\"Loss\")\n",
    "plt.xlabel(\"Epochs\")\n",
    "plt.title(\"Loss curve\")\n",
    "plt.legend()"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "validation_loss/"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "images_gpu = images.to(device)\n",
    "captions_gpu = [caption.to(device) for caption in captions]\n",
    "captions_padded=nn.utils.rnn.pad_sequence(captions_gpu,batch_first=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "validation_image=validation_image.to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "validation_image=validation_image.view(1,3,224,224)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "actual_outputs=validation_caption.cpu().numpy().tolist()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "encoder.eval()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "from torch import nn\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "len(valid_dataset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.font_manager as fm\n",
    "prop = fm.FontProperties(fname='Kalpurush.ttf')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "encoder.eval()\n",
    "decoder.eval()\n",
    "count=0\n",
    "for validation_image,validation_caption in valid_dataset:\n",
    "    features=encoder(validation_image.unsqueeze(0))\n",
    "    start =0\n",
    "    start_tensor=torch.tensor(start)\n",
    "    start_tensor.to(\"cpu\")\n",
    "    word_embedding= decoder.word_embeddings(start_tensor)\n",
    "    word_embedding.size()\n",
    "    batch_size=features.shape[0]\n",
    "    word_alphas=[]\n",
    "    bangla_words=[]\n",
    "\n",
    "    decoder.hidden,decoder.cell=decoder.init_hidden(batch_size)\n",
    "\n",
    "    while True:\n",
    "        attention_feature,alpha = decoder.attention(features,decoder.hidden)\n",
    "\n",
    "        word_alphas.append(alpha)\n",
    "\n",
    "        gate= decoder.sigmoid(decoder.f_beta(decoder.hidden))\n",
    "\n",
    "        attention_feature=attention_feature*gate\n",
    "\n",
    "        embedding= torch.cat((attention_feature,word_embedding.unsqueeze(0)),dim=1)\n",
    "\n",
    "        hidden=decoder.hidden\n",
    "        cell=decoder.cell\n",
    "        hidden,cell=decoder.decode_step(embedding,(hidden,cell))\n",
    "        decoder.hidden,decoder.cell=hidden,cell\n",
    "\n",
    "        pred_word= decoder.linear(decoder.hidden)\n",
    "\n",
    "        pred_word = nn.functional.softmax(pred_word)\n",
    "\n",
    "        word_indice=torch.max(pred_word,dim=1)[1]\n",
    "        bangla_words.append(vocab.idx2word[word_indice.item()])\n",
    "        word_embedding = decoder.word_embeddings(word_indice[0])\n",
    "        if word_indice==1:\n",
    "            break\n",
    "    plt.figure()\n",
    "    plt.imshow(validation_image.cpu().numpy().transpose((1,2,0)))\n",
    "    plt.title(\" \".join(bangla_words), fontproperties=prop)\n",
    "    count=count+1\n",
    "    if count>99:\n",
    "        break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\"\"\" Define the feattention_feature behavior of the model \"\"\"\n",
    "input_captions=[[self.word_embeddings(word) for word in single_caption[:-1]] for single_caption in captions]\n",
    "pred_captions=[]\n",
    "caption_alphas=[]\n",
    "\n",
    "#         input_captions=nn.utils.rnn.pad_sequence(input_captions,batch_first=True)\n",
    "\n",
    "#         input_captions_lengths=[len(input_caption)+1 for input_caption in input_captions]\n",
    "#         embeddings = self.word_embeddings(input_captions) # embeddings new shape : (batch_size, captions length - 1, embed_size)\n",
    "\n",
    "batch_size = features.shape[0] # features is of shape (batch_size, embed_size)\n",
    "for i,input_caption in enumerate(input_captions):\n",
    "    self.hidden,self.cell=self.init_hidden(1)\n",
    "    feature=features[i]\n",
    "    #print(\"feature size\")\n",
    "    #print(feature.size())\n",
    "\n",
    "    #print(self.hidden.size())\n",
    "    #print(self.cell.size())\n",
    "    pred_words=[]\n",
    "    word_alphas=[]\n",
    "    for input_word in input_caption:\n",
    "        attention_feature,alpha= self.attention(feature.view(1,feature.size(0),feature.size(1)),self.hidden)\n",
    "        word_alphas.append(alpha)\n",
    "        gate=self.sigmoid(self.f_beta(self.hidden))\n",
    "        #print(\"attention feature\")\n",
    "        #print(attention_feature.size())\n",
    "        #print(\"gate \")\n",
    "        #print(gate.size())\n",
    "        attention_feature=attention_feature*gate\n",
    "        #print(attention_feature.size())\n",
    "        #print(input_word.size())\n",
    "        embedding= torch.cat((attention_feature,input_word.unsqueeze(0)),dim=1)\n",
    "        hidden=self.hidden\n",
    "        cell=self.cell\n",
    "        hidden,cell=self.decode_step(embedding,(hidden,cell))\n",
    "        #print(\"hidden and cell\")\n",
    "        #print(hidden.size())\n",
    "        #print(cell.size())\n",
    "        self.hidden,self.cell =hidden,cell\n",
    "        pred_word=self.linear(self.dropout(self.hidden))\n",
    "        pred_words.append(pred_word)\n",
    "    pred_captions.append(torch.cat(pred_words))\n",
    "    caption_alphas.append(word_alphas)\n",
    "pred_captions_padded=nn.utils.rnn.pad_sequence(pred_captions,batch_first=True)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def sample(features):\n",
    "    \" accepts pre-processed image tensor (features) and returns predicted sentence (list of tensor ids of length max_len) \"\n",
    "\n",
    "\n",
    "    output = []\n",
    "    batch_size = features.shape[0] # batch_size is 1 at inference, features shape : (1, 1, embed_size)\n",
    "    hidden = decoder.init_hidden(batch_size) # Get initial hidden state of the LSTM\n",
    "\n",
    "    while True:\n",
    "        lstm_out, hidden = decoder.lstm(features, hidden) # lstm_out shape : (1, 1, hidden_size)\n",
    "        outputs = decoder.linear(lstm_out)  # outputs shape : (1, 1, vocab_size)\n",
    "        outputs= nn.functional.softmax(outputs,dim=2)\n",
    "        outputs = outputs.squeeze(1) # outputs shape : (1, vocab_size)\n",
    "        _, max_indice = torch.max(outputs, dim=1) # predict the most likely next word, max_indice shape : (1)\n",
    "\n",
    "        output.append(max_indice.cpu().numpy()[0].item()) # storing the word predicted\n",
    "\n",
    "        if (max_indice == 0):\n",
    "            # We predicted the <end> word, so there is no further prediction to do\n",
    "            break\n",
    "\n",
    "        ## Prepare to embed the last predicted word to be the new input of the lstm\n",
    "        features = decoder.word_embeddings(max_indice) # features shape : (1, embed_size)\n",
    "        features = features.unsqueeze(1) # features shape : (1, 1, embed_size)\n",
    "\n",
    "    return output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "outputs=sample(features.view(1,1,4096))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for output in outputs:\n",
    "    print(vocab.idx2word[output])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for output in actual_outputs:\n",
    "    print(vocab.idx2word[output])"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "nn.functional.softmax()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
